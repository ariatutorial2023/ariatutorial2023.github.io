
<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<!-- css file modified from https://taesung.me/SwappingAutoencoder/ -->
<style type="text/css">
  body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1000px;
  }
  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
    5px 5px 0 0px #fff, /* The second layer */
    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
    10px 10px 0 0px #fff, /* The third layer */
    10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
    15px 15px 0 0px #fff, /* The fourth layer */
    15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
    20px 20px 0 0px #fff, /* The fifth layer */
    20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
    25px 25px 0 0px #fff, /* The fifth layer */
    25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }

  .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

    margin-left: 10px;
    margin-right: 45px;
  }

  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
    5px 5px 0 0px #fff, /* The second layer */
    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
    10px 10px 0 0px #fff, /* The third layer */
    10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<!-- ----------------------------------------------------------------------------- -->

<html>

  <!-- ----------------------------------------------------------------------------- -->

  <head>
    <title>CVPR 2023 Tutorial: Hands-on Egocentric research with Project Aria, from Meta</title>
    <meta property="og:image" content="" />
    <meta property="og:title" content="CVPR 2023 Tutorial: Hands-on Egocentric research with Project Aria, from Meta" />
  </head>

  <!-- ----------------------------------------------------------------------------- -->

  <body>
    <br>
    <!-- <center> -->
      <center><span style="font-size:45px">CVPR 2023 Tutorial: Hands-on Egocentric research with Project Aria, from Meta</span></center>
      <br>
      <center><span style="font-size:25px">Monday 19 June 2023, afternoon session</span></center>
      <br>
      <center><img src="./resources/project_aria_animated.gif" width="500"></center></br>

      <table align=center width=900px>
        <tr>
          <td align=center width=250px>
            <span style="font-size:20px"><a href="https://scholar.google.co.uk/citations?user=MhowvPkAAAAJ&hl=en">Richard Newcombe</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=7Ra6nSoAAAAJ&hl=en">Xiaqing Pan</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;<span style="font-size:20px"><a href="https://scholar.google.co.uk/citations?user=AGoNHcsAAAAJ&hl=en">Vasileios Balntas</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;<br>
            <span style="font-size:20px"><a href="https://uk.linkedin.com/in/edwardrichardmiller">Edward Miller</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;  
            <span style="font-size:20px"><a href="https://scholar.google.fr/citations?user=8u1nmVUAAAAJ&hl=en">Pierre Moulon</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://www.linkedin.com/in/guptaprince/">Prince Gupta</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
            <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=hS6M138AAAAJ&hl=en">Rawal Khirodkar</a><sup>2</sup></span> 
          </td>
        </tr>
      </table>

      <table align=center width=1000px>
        <tr>
          <td align=center width=100px>
            <center>
              <span style="font-size:20px"></span>
            </center>
          </td>
          <td align=center width=400px>
            <center>
              <span style="font-size:20px"><sup>1</sup>Meta Reality Labs Research</span>
            </center>
          </td>
          <td align=center width=400px>
            <center>
              <span style="font-size:20px"><sup>2</sup>Carnegie Mellon University</span>
            </center>
          </td>
          <td align=center width=100px>
            <center>
              <span style="font-size:20px"></span>
            </center>
          </td>
        </tr>
      </table>

      <br>
      <hr>

    <!-- ----------------------------------------------------------------------------- -->

    <table align=center width=900px>
      <center><h2>Abstract</h2></center>
      <tr>
        <a href="https://www.projectaria.com/" target="_new">Project Aria</a> is a research device from Meta, which is worn like a regular pair of glasses, and enables researchers to study the future of always-on egocentric perception.</br></br>

In this tutorial, we will introduce two exciting new datasets from Project Aria: <a href="https://www.projectaria.com/datasets/adt/" target="_new">Aria Digital Twin</a>, a real-world dataset with hyper-accurate digital counterpart; and <a href="https://www.projectaria.com/datasets/ase/" target="_new">Aria Synthetic Environments</a>, a procedurally-generated synthetic Aria dataset for large-scale ML research. Each dataset will be presented with corresponding challenges, which we believe will be powerful catalysts for research.</br></br>

In addition to introducing new datasets and research challenges, we will also provide a hands-on demonstration of newly open-sourced tools for working with Project Aria, and demonstrate how the Project Aria ecosystem can be used to accelerate open research into egocentric perception tasks such as visual and non-visual localization and mapping, static and dynamic object detection and spatialization, human pose and eye-gaze estimation, and building geometry estimation.<br> <br>


        Learn more about associated challenges and datasets at <a href="https://www.ProjectAria.com" target="_new">ProjectAria.com.</a><br><br>

        <center><img src="./resources/adt.gif" width="500"><img src="./resources/ase.gif" width="500"></center></br>
      </tr>
      <br>

      <hr>

          <!-- ----------------------------------------------------------------------------- -->

    <table align=center width=900px>
      <center><h2>Agenda</h2></center>
      <tr>
        <p style="text-align: center;">SECTION ONE: An Introduction to Project Aria</p>

        13:30 Always on machine perception, by Richard Newcombe</br>

        13:45 Introduction to Project Aria, by Prince Gupta</br>

        <p style="text-align: center;">SECTION TWO: Aria Research Kit</p>

        14:00 Overview of Aria Research Kit & Partner Program, by Sach Lakhavani</br>

        14:12 Machine Perception Services, by Jakob Engel</br>

        14:24 Open Source SW ecosystem and roadmap overview, by Carl Ren</br>

        14:36 Hands-on demonstration via Jupyter notebook & CLI, by Vijay Baiyya</br></br>

        14:50 BREAK (25 minutes)</br>
        
        <p style="text-align: center;">SECTION THREE: New Open Datasets, Models and Challenges</p>

        15:15 Overview of Open Science Initiatives, by Edward Miller</br>

        15:25 New Dataset & Challenge: Aria Digital Twin, Xiaqing Pan and Carl Ren</br>

        15:45 New Dataset & Challenge: Aria Synthetic Environments, by Vasileios Balntas</br>

        16:05 Partner Highlights: EgoHumans Dataset from CMU for inferring human position, by Rawal Khirodkar</br>

        <p style="text-align: center;">SECTION FOUR: Closing remarks</p>

        16:25 Closing Remarks & Joint Q&A</br>
        </br>
      </tr>
      <br>

      <hr>

    <!-- ----------------------------------------------------------------------------- -->

      <center><h2>Questions?</h2></center>

      <table align=center width=900px>
        <tr>
          Visit the <a href="https://www.ProjectAria.com" target="_blank">Project Aria website</a> or email <a href="mailto:projectaria@meta.com">projectaria@meta.com</a> to get more information on the project.</br></br>

          Academic and industrial research institutions interested in participating in Project Aria can submit their proposals <a href="https://fb.me/AriaPartnerInterest">here</a>.</br>
        </tr>
      </table>

      <br>

  </body>

</html>
